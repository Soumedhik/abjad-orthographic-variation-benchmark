# Beyond Standardized Benchmarks: Orthographic Variation Benchmark

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Accepted at AbjadNLP 2026](https://img.shields.io/badge/AbjadNLP-2026--Oral-green)](https://wp.lancs.ac.uk/abjad/)

This repository contains the official implementation for the paper **"Beyond Standardized Benchmarks: Quantifying LLM Degradation Under Realistic Orthographic Variation in Abjad Languages"** â€” accepted to AbjadNLP 2026 as an *oral* presentation (â‰ˆ20% acceptance rate). Conference: https://wp.lancs.ac.uk/abjad/

## ğŸ“„ Paper Abstract

Large language models (LLMs) perform well on multilingual natural language inference benchmarks, but real-world low-resource text contains orthographic variation absent from curated evaluations. We present the first systematic study of LLM robustness to artificially generated orthographic variation across Arabic, Urdu, Swahili, and English using modified XNLI benchmarks. Evaluating Llama 3.3 70B, Llama 3.1 8B, Qwen 2.5 32B, and GPT-OSS models across 80 language-condition pairs, we observe substantial performance degradation, with accuracy drops of up to 41% under romanization and up to 61% under code-switching. Smaller models fail catastrophically, with Llama 8B achieving 13% accuracy on fully romanized Urdu. Error analysis identifies label bias, out-of-vocabulary issues, and script asymmetries as key failure modes.

## ğŸ¯ Key Findings

- **Orthographic fragility is severe**: Accuracy drops 8-24% under romanization and 15-41% under code-switching
- **Model size matters**: 70B model degrades gracefully, while 8B model collapses catastrophically
- **Universal limitation**: Even high-resource English degrades 41% under code-switching
- **Arabic shows robustness**: Diacritics removal causes minimal degradation (~2%)

## ğŸ“Š Evaluated Models

- **Llama 3.3 70B Versatile** (Meta)
- **Llama 3.1 8B Instant** (Meta)
- **Qwen 2.5 32B Instruct** (Alibaba)
- **GPT-OSS 20B** (OpenAI-style)
- **GPT-OSS 120B MoE** (Mixture-of-Experts)

## ğŸŒ Languages & Conditions

### Arabic (ar)
- **Clean**: Original XNLI text
- **No diacritics**: All vowel diacritics removed
- **Partial diacritics**: Only final-position case markers retained

### Urdu (ur)
- **Clean**: Original Perso-Arabic script
- **R25/R50/R100**: Romanization at 25%, 50%, 100% word-level
- **M25/M50**: Mixed-script code-switching with English at 25%, 50%

### English (en)
- **Clean**: Original Latin script
- **M25/M50**: Reverse code-switching with Urdu tokens at 25%, 50%

### Swahili (sw)
- **Clean**: Original Latin script
- **Romanized**: Maintained Latin script (control)
- **M25/M50**: Code-switching with English at 25%, 50%

**Total**: 16 language-condition pairs Ã— 5 models = 80 model-language-condition combinations

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or higher
- Groq API key(s) for model inference

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/Soumedhik/abjad-orthographic-variation-benchmark.git
   cd abjad-orthographic-variation-benchmark
   ```

2. **Create and activate a virtual environment**:
   ```bash
   python -m venv .venv
   
   # On Windows
   .venv\Scripts\activate
   
   # On Linux/Mac
   source .venv/bin/activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure API keys**:
   ```bash
   copy .env.example .env  # Windows
   # or
   cp .env.example .env    # Linux/Mac
   ```
   
   Edit `.env` and set your Groq API keys:
   ```
   GROQ_API_KEYS=your_key_1,your_key_2,your_key_3
   ```

### Dataset Preparation

The benchmark expects XNLI data in CSV format. Each file should be named `{language}_{split}.csv` and contain columns: `premise`, `hypothesis`, `label`.

```
data/
â”œâ”€â”€ ar_test.csv
â”œâ”€â”€ ur_test.csv
â”œâ”€â”€ en_test.csv
â””â”€â”€ sw_test.csv
```

You can download XNLI from the [official repository](https://github.com/facebookresearch/XNLI) or [Hugging Face](https://huggingface.co/datasets/xnli).

## ğŸ“¦ Usage

### Run Full Benchmark

```bash
python scripts/run_benchmark.py \
    --dataset-dir ./data \
    --results-dir ./results \
    --max-examples 40
```

### Compute Performance Deltas

After running the benchmark, calculate degradation metrics:

```bash
python scripts/compute_deltas.py \
    --benchmark ./results/benchmark.csv \
    --out-dir ./results
```

### Configuration Options

All settings can be configured via CLI flags or environment variables (`.env` file):

| Parameter | Environment Variable | Default | Description |
|-----------|---------------------|---------|-------------|
| `--dataset-dir` | `DATASET_DIR` | `../input/xnli-multilingual-nli-dataset` | Path to XNLI CSV files |
| `--results-dir` | `RESULTS_DIR` | `./results` | Output directory |
| `--eval-split` | `EVAL_SPLIT` | `test` | Dataset split (train/validation/test) |
| `--languages` | `LANGUAGES` | `ar,ur,en,sw` | Comma-separated language codes |
| `--max-examples` | `MAX_EXAMPLES_PER_CONDITION` | `40` | Examples per condition |
| `--requests-per-minute` | `REQUESTS_PER_MINUTE` | `60` | API rate limit |
| `--write-traces` | `WRITE_TRACES` | `0` | Write per-example traces (0/1) |

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_benchmark.py       # Main evaluation script
â”‚   â””â”€â”€ compute_deltas.py      # Calculate performance degradation
â”œâ”€â”€ src/
â”‚   â””â”€â”€ orthographic_nli/
â”‚       â”œâ”€â”€ __init__.py        # Package initialization
â”‚       â”œâ”€â”€ config.py          # Configuration management
â”‚       â”œâ”€â”€ data.py            # XNLI data loading
â”‚       â”œâ”€â”€ variants.py        # Orthographic variant generation
â”‚       â”œâ”€â”€ groq_client.py     # Groq API interface
â”‚       â”œâ”€â”€ evaluate.py        # Model evaluation logic
â”‚       â”œâ”€â”€ metrics.py         # Performance metrics
â”‚       â””â”€â”€ traces.py          # Detailed trace logging
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env.example               # Environment configuration template
â”œâ”€â”€ LICENSE                    # MIT License
â””â”€â”€ README.md                  # This file
```

## ğŸ”¬ Technical Details

### Orthographic Transformations

1. **Diacritics Manipulation** (Arabic):
   - Strip all Unicode combining marks (U+064B-U+0652)
   - Retain only final-position case markers

2. **Romanization** (Urdu):
   - Character-level transliteration using Urdu romanization conventions
   - Dose-response design: 25%, 50%, 100% word-level application

3. **Code-Switching** (All languages):
   - Random token replacement from donor language
   - Maintains semantic plausibility by sampling from parallel XNLI splits

### Evaluation Metrics

- **Strict Exact-Match Accuracy**: Normalized prediction must match gold label exactly
- **Macro F1 Score**: Averaged across entailment, neutral, and contradiction classes
- **Confusion Matrices**: Per model-language-condition for error analysis

## ğŸ“ˆ Results

Key results from the paper (summary across all models):

| Perturbation Type | Avg. Accuracy Drop |
|-------------------|-------------------|
| Arabic Diacritic Removal | -2.4% |
| Urdu Romanization (Full) | -28.6% |
| Code-Switching (50% Mix) | -41.2% |

See the paper for detailed results, confusion matrices, and error analysis.

## ğŸ“ Citation

If you use this code or data in your research, please cite:

```bibtex
@article{mandal2026orthographic,
  title={Beyond Standardized Benchmarks: Quantifying LLM Degradation Under Realistic Orthographic Variation in Abjad Languages},
  author={Mandal, Shibam and Bharati, Soumedhik and Ghosh, Swarup Kr and Mondal, Sayani},
  journal={arXiv preprint},
  year={2026},
  institution={Sister Nivedita University}
}
```

## ğŸ‘¥ Authors

- **Shibam Mandal** - Sister Nivedita University - [shibammandal603@gmail.com](mailto:shibammandal603@gmail.com)
- **Soumedhik Bharati** - Sister Nivedita University - [soumedhikbharati@gmail.com](mailto:soumedhikbharati@gmail.com)
- **Swarup Kr Ghosh** - Sister Nivedita University - [swarupg1@gmail.com](mailto:swarupg1@gmail.com)
- **Sayani Mondal** - Sister Nivedita University - [sayani.mondal9@gmail.com](mailto:sayani.mondal9@gmail.com)

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- XNLI dataset creators for providing multilingual NLI benchmarks
- Groq for providing API access to LLMs
- Meta, Alibaba, and OpenAI for open-source model releases

## ğŸ”— Links

- **Paper (AbjadNLP 2026)**: https://wp.lancs.ac.uk/abjad/ (accepted â€” oral; â‰ˆ20% acceptance)
- **Dataset**: [XNLI](https://github.com/facebookresearch/XNLI)
- **Models**: [Groq Cloud](https://groq.com/)

## âš ï¸ Limitations

- Evaluation limited to 40 examples per condition due to API costs
- Programmatic orthographic variants approximate but don't perfectly match natural variation
- Results specific to Groq API inference; may vary with self-hosted deployments
- Focus on NLI; generative task robustness may differ

## ğŸ› Issues & Contributions

Found a bug or have a suggestion? Please [open an issue](https://github.com/Soumedhik/abjad-orthographic-variation-benchmark/issues).

Contributions are welcome! Please ensure code follows existing style and includes appropriate tests/documentation.

---

**Made with â¤ï¸ for multilingual NLP research**
